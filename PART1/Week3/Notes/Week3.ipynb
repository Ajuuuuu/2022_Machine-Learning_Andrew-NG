{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e692a7f9",
   "metadata": {},
   "source": [
    "## 1.1 Motivation 动机与目的\n",
    "\n",
    "    Classification \"binary\" classification 二元分类\n",
    "    How to do classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14796603",
   "metadata": {},
   "source": [
    "## 1.2 Logistic Regression 逻辑回归\n",
    "    简单来说， 逻辑回归（Logistic Regression）是一种用于解决二分类（0 or 1）问题的机器学习方法，用于估计某种事物的可能性。\n",
    "    虽然叫逻辑回归，但是却是用于解决分类问题的\n",
    "    probably the single most widely used classification alogorithm\n",
    "    \n",
    "    二元分类问题去拟合如下曲线\n",
    "![img1](./img/01.png)\n",
    "\n",
    "    sigmoid function(aka logistic function)\n",
    "![img2](./img/02.png)\n",
    "\n",
    "    the sigmoid function is： \n",
    "$$ g(z) = \\frac{1}{1+e^{-z} } $$\n",
    "\n",
    "    and the logistic regression model is:\n",
    "$$ f_{\\vec{w},b }(\\vec{X} ) =g(\\vec{w}\\cdot \\vec{x}+b) = \\frac{1}{1+e^{-(\\vec{w}\\cdot \\vec{x}+b)} } $$\n",
    "\n",
    "$$ f_{\\vec{w},b }(\\vec{X} ) $$ \n",
    "\n",
    "    往往得到的是y is 1的可能性  比如得到结果0.7、0.3之类的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd65853b",
   "metadata": {},
   "source": [
    "## 1.3 Decision Boundary 决策边界\n",
    "    决策边界（Decision Boundary）是用于分类模型的一个概念，它在特征空间中定义了一个边界，用于区分不同类别的数据。给定一个输入特征向量，模型会根据这个向量所在决策边界的哪一侧来预测其属于哪个类别。决策边界是分类器试图学习的目标，它试图找到一个能够最大程度地正确分类训练数据的边界。（此段文字来自ChatGPT4.0\n",
    "\n",
    "    对于二元分类的问题，我们通常只希望得到1或者0两种结果\n",
    "    而 y_hat 却是一个等于1的可能性  我们认为\n",
    "    当 y_hat >= 0.5 是认为结果是1    y_hat < 0.5是被认为结果是0\n",
    "    \n",
    "    y_hat = g(z) >= 0.5  -->   z = wx + b >= 0\n",
    "    y_hat = g(z) < 0.5   -->  z = wx + b < 0\n",
    "    \n",
    "$$ \\vec{w}\\cdot \\vec{x}+b = 0 $$\n",
    "\n",
    "    This line is also called The Decision Boundary\n",
    "    \n",
    "    With Polynomial features(多项式特征), We can get very complex decision boundaries\n",
    "    In other words, it means logistic regression can learn to fit pretty complex data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b12fed6",
   "metadata": {},
   "source": [
    "## 2.1 Cost Function for Logistic Regression 逻辑回归里的代价函数\n",
    "\n",
    "    WHAT'S THE DATA?\n",
    "![img3](./img/03.png)\n",
    "\n",
    "    The Squared error cost:\n",
    "![img4](./img/04.png)\n",
    "\n",
    "    但是在逻辑回归中再次使用平方误差损失，得到的损失函数不是一个凸函数\n",
    "    也就意味着 我们在使用梯度下降可能会找到局部最小值，无法找到最小值，或者甚至无法收敛\n",
    "![img5](./img/05.png)\n",
    "    \n",
    "    那么我们使用以下损失函数：\n",
    "![img6](./img/06.png)\n",
    "    \n",
    "    我们先看如下的函数图 来理解上述损失函数\n",
    "![img7](./img/07.png)\n",
    "\n",
    "    我们知道f(x)的值是在0-1之间的，所以我们只观察上述函数图像里x在0-1区间上的情况\n",
    "    当 y = 1 时，我们应该让越接近1的值有越小的损失，对应图中的蓝色图像\n",
    "    当 y = 0 时，我们让越接近0的值有越小的损失，当f接近0，1-f就接近1，所以也对应蓝色图像\n",
    "    \n",
    "    这样的损失函数就是一个凸函数，我们可以使用梯度下降来找到最小值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39ec6d9",
   "metadata": {},
   "source": [
    "## 2.2 Simplified Cost Fuction 简化逻辑回归代价函数\n",
    "    损失函数\n",
    "![img8](./img/06.png)\n",
    "\n",
    "    而对于上述的损失函数，我们可以做如下简化\n",
    "![img9](./img/08.png)\n",
    "\n",
    "    将所有的损失函数相加，我们得到如下的代价函数：\n",
    "![img10](./img/09.png)\n",
    "\n",
    "    我们通过最大似然估计得到的此代价函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9215cf33",
   "metadata": {},
   "source": [
    "## 3.1 GD Implementation 实现梯度下降\n",
    "    \n",
    "    How to updata w and b\n",
    "![img11](./img/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead2400a",
   "metadata": {},
   "source": [
    "## 4.1 The Problem of Overfitting 过拟合问题\n",
    "    欠拟合（Underfitting）是指机器学习模型在训练数据上表现不佳的现象。这通常意味着模型无法捕捉到数据中的基本结构或者模式，导致在训练集和测试集上都表现较差。\n",
    "    \n",
    "    过拟合（Overfitting）是指机器学习模型在训练数据上表现很好，但在新的、未见过的数据（例如测试集）上表现较差的现象。过拟合发生时，模型过于复杂，以至于它捕捉到了训练数据中的噪声，而非真实的潜在关系。这导致模型在新数据上的泛化能力较差。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bed97b",
   "metadata": {},
   "source": [
    "## 4.2 Addressing Overfitting 解决过拟合\n",
    "- Collect more training examples\n",
    "\n",
    "- Select features to include/exclude(feature selection)\n",
    "\n",
    "- Regularization 正则化\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aacc051",
   "metadata": {},
   "source": [
    "## 4.3 Cost Function with Regularization 正则化\n",
    "来自ChatGPT4.0:\n",
    "\n",
    "    机器学习中的正则化（Regularization）是一种用于降低模型复杂度、防止过拟合的技术。正则化通过向模型的目标函数（损失函数）添加一个额外的惩罚项来实现。这个惩罚项会对模型的参数施加约束，从而限制模型复杂度。正则化有助于提高模型在未见过的数据上的泛化能力。\n",
    "\n",
    "    常见的正则化方法有两种：L1 正则化（Lasso）和 L2 正则化（Ridge）。\n",
    "\n",
    "    L1 正则化（Lasso）：在目标函数中添加参数的绝对值之和作为惩罚项。L1 正则化导致许多参数变为零，从而产生一个稀疏模型。这有助于进行特征选择，因为模型会选择与目标变量最相关的特征，而忽略其他不重要的特征。\n",
    "\n",
    "    L2 正则化（Ridge）：在目标函数中添加参数的平方和作为惩罚项。与 L1 正则化不同，L2 正则化不会将参数完全压缩为零，而是将它们压缩到较小的值。这有助于防止过拟合，同时保留多数特征的影响。\n",
    "\n",
    "    有时，L1 和 L2 正则化会结合在一起使用，称为弹性网（Elastic Net）正则化。这种方法结合了 Lasso 和 Ridge 的优点，既能进行特征选择，又能保留多数特征的影响。\n",
    "\n",
    "    在使用正则化方法时，需要选择一个合适的正则化系数。正则化系数过大，可能导致模型过于简化，引发欠拟合；正则化系数过小，可能无法充分防止过拟合。通常，可以通过交叉验证（Cross-Validation）来选择最佳的正则化系数。\n",
    "\n",
    "\n",
    "![img12](./img/11.png)\n",
    "\n",
    "    这里我们加入了惩罚项，要想使cost function变小，只有将 w3 与 w4 的值变小，这样也就解决了过拟合的问题\n",
    "    \n",
    "![img13](./img/12.png)\n",
    "   \n",
    "    因为我们不知道哪些features是重要的，哪些是不重要的，所以我们对所有的features都加上惩罚项，引入的参数 lambda \n",
    "    \n",
    "    同时对于lambda的大小选择也是很重要的，若lambda很小，则起不到正则化的作用；若lambda很大很大，则会导致模型欠拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac20f2",
   "metadata": {},
   "source": [
    "## 4.4 Regularized Linear Regression 用于线性回归的正则方法\n",
    "    我们在代价函数后加上了惩罚项，得到了以下的代价函数\n",
    "\n",
    "![img14](./img/13.png)\n",
    "\n",
    "    \n",
    "    在代价函数改变后，自然参数的迭代公式也会改变：\n",
    "    \n",
    "![img15](./img/14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d5277",
   "metadata": {},
   "source": [
    "## 4.5 Regularized Logistic Regression 用于逻辑回归的正则方法\n",
    "\n",
    "![img16](./img/15.png)\n",
    "\n",
    "    对于参数的迭代，其公式和上述线性回归的正则方法得到的公式是一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090fbb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
