{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e45b81a4",
   "metadata": {},
   "source": [
    "## 2.1 什么是机器学习\n",
    "监督学习\n",
    "无监督学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa17a6ad",
   "metadata": {},
   "source": [
    "## 2.2 Supervised Learning 监督学习\n",
    "监督学习需要对训练数据进行标记，以表示该数据的正确输出。算法通过对训练数据的学习来构建一个模型，以便对未知数据进行预测。监督学习的主要任务是回归或分类。\n",
    "   #### 回归是一种机器学习的任务，它的目的是预测一个连续的标签（例如价格）。回归算法通过学习输入特征与标签的关系，来生成一个连续的输出。\n",
    "   #### 分类是一种机器学习任务，目的是预测离散的标签（例如邮件是垃圾邮件还是非垃圾邮件）。分类算法通过学习输入特征与标签的关系，来生成一个离散的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d60b9df",
   "metadata": {},
   "source": [
    "## 2.3 Unsupervised Learning 无监督学习\n",
    "    无监督学习是一种机器学习方法，它不需要明确的目标变量或标签，而是通过从数据中提取有用信息来学习模型。 无监督学习的目的是找出数据之间的关系，并对数据进行分组，从而生成一个无监督的数据模型。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff5bbd2",
   "metadata": {},
   "source": [
    "## 3.1 Linear Regression 线性回归\n",
    "    training set:训练集\n",
    "    x = \"input\" variable or feature\n",
    "    y = \"output\" variable or \"target\" variable\n",
    "    m = number of training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046042b6",
   "metadata": {},
   "source": [
    "## 3.2 线性回归part2\n",
    "    training set + learning algorithm -> function\n",
    "    the function will output y_hat with a new x\n",
    "    \n",
    "    But how to represent the function?\n",
    "#### ${f}_{w,b}=wx+b $ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f94d78f",
   "metadata": {},
   "source": [
    "## 3.3 Cost Function 代价函数\n",
    "    Cost function（代价函数）是机器学习中一个重要的概念。它是一个评估模型的指标，表示模型的预测值与真实值的差距。\n",
    "    In the function w,b are parameters\n",
    "    \n",
    "    Find w and b which make y_hat is close to y\n",
    "#### cost function $ J\\left ( {w,b} \\right ) $ = $ \\frac{\\sum ^{m}_{i=1} {\\left ( {{y}^{(i)}_{hat}-{y}^{(i)}} \\right )^{2}} }{2m}$  this function also called squared error cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5f0b5c",
   "metadata": {},
   "source": [
    "## 3.4 What cost function really doing? 理解代价函数\n",
    "    Find w and b that makes J as small as possible\n",
    "    \n",
    "    We first consider y = wx\n",
    " #### the cost function is: $ J\\left ( {w} \\right ) $ = $ \\frac{\\sum ^{m}_{i=1} {\\left ( {{wx}^{(i)}-{y}^{(i)}} \\right )^{2}} }{2m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d95db43",
   "metadata": {},
   "source": [
    "## 3.5 Visualizing the Cost Function 可视化代价函数\n",
    "## 3.6 可视化举例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3411f0d9",
   "metadata": {},
   "source": [
    "## 4.1 Gradient Descent 梯度下降\n",
    "    1. Start with some w,b\n",
    "    2. Keep changing w,b to reduce J(w,b)\n",
    "    3. Until we settle at or near a minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff41dae",
   "metadata": {},
   "source": [
    "## 4.2 Implementing GD 梯度下降的实现\n",
    " $ w=w-\\alpha \\frac {\\partial } {\\partial w}J(w,b) $\n",
    " \n",
    " $ \\frac {\\partial } {\\partial w}J(w,b) $ decide the direction\n",
    " and the $ \\alpha $ means learning rate\n",
    "\n",
    "$ w=w-\\alpha \\frac {\\partial } {\\partial w}J(w,b) $ \n",
    "\n",
    "$ b=b-\\alpha \\frac {\\partial } {\\partial b}J(w,b) $      \n",
    "    repeat the them until convergence(收敛)\n",
    "    \n",
    "    \n",
    "    the correct way : Simultaneous update(同时更新)\n",
    "    tmp_w = w - ...\n",
    "    tmp_b = b - ...\n",
    "    w = tmp_w\n",
    "    b = tmp_b\n",
    "    do not use the tmp_w to count the new tmp_b\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875cc725",
   "metadata": {},
   "source": [
    "## 4.3 GD Intuition 理解梯度下降\n",
    "$ \\alpha $ : learning rate  \n",
    "\n",
    "$\\alpha$ >0\n",
    "\n",
    "    同时对于这里的偏导，我们想象对于b一定时，对于所有的w存在于同一个平面\n",
    "    此时平面上的w的图像，我们可以沿着梯度和步长得到它的极小值\n",
    "    这就是求偏导的原因"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69789c49",
   "metadata": {},
   "source": [
    "## 4.4 Learning Rate 学习率\n",
    "    If it's too small: need a lots of steps    be very slow\n",
    "    If it's too large: you may never reach minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad4d068",
   "metadata": {},
   "source": [
    "## 4.5 GD for Linear Regression 用于线性回归的梯度下降\n",
    "![img1](./img/01.png)\n",
    "\n",
    "    将式子依次带入 然后求偏导可以得到\n",
    "    \n",
    "![img2](./img./02.png)\n",
    "\n",
    "    然后重复这两式，直至收敛\n",
    "    \n",
    "\n",
    "    同时，对于Squared error cost function\n",
    "    此代价函数是一个凸函数，意味着有且只有一个最小值，没有局部极小值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0253c01",
   "metadata": {},
   "source": [
    "## 4.6 Running GD 运行梯度下降\n",
    "    \"Batch\" GD : 批量梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0089658",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
