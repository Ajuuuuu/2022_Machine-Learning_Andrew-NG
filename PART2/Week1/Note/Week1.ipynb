{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "701a40eb",
   "metadata": {},
   "source": [
    "## 1.2 Neurons and the brain 神经元和大脑\n",
    "    Origins: Algorithms that try to mimic the brain\n",
    "    \n",
    "    With traditional machine-learning algorithms, such as logistic regression and linear regression, even you fed those algorithms more data, it was very diffcult to make the preformance to keep on going up.\n",
    "    \n",
    "    GPU if powerful for DL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758f2856",
   "metadata": {},
   "source": [
    "## 1.3 Demand Prediction 需求预测\n",
    "    activation 激活\n",
    "    \n",
    "    a logistic regression can be considered as a single neuron\n",
    "    and it's a very simple neuron network\n",
    "    \n",
    "![img1](./img/01.png)\n",
    "\n",
    "    the input layer, hidden layer and output layer\n",
    "    \n",
    "    multilayer perceptron 多层感知机 也就是一种神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c14213",
   "metadata": {},
   "source": [
    "## 1.4 Eample Recognizing Images 举例-图像感知\n",
    "![img2](./img/02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5d0a3b",
   "metadata": {},
   "source": [
    "## 2.1 Neural network layer 神经网络中的网络层\n",
    "    一层神经元网络\n",
    "![img3](./img/03.png)\n",
    "![img4](./img/04.png)\n",
    "![img5](./img/05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5425c0",
   "metadata": {},
   "source": [
    "## 2.2 More complex neural networks 更复杂的神经网络\n",
    "    我们说神经网络有多少层时，一般只计数隐藏层与输出层，不计入输入层\n",
    "    \n",
    "    Activation Function 激活函数\n",
    "    Activation Function（激活函数）是神经网络中的一种函数，用于在神经元接收输入后产生输出。\n",
    "    \n",
    "    Now we know only one type of activation function -- sigmoid function\n",
    "    \n",
    "![img6](./img/06.png)\n",
    "\n",
    "    And we also call the input \n",
    "$$ \\vec{X} = \\vec{a}^{[0]} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4e3c05",
   "metadata": {},
   "source": [
    "## 2.3 Inference:making predictions (forward propagation) 神经网络向前传播\n",
    "    前向传播（Forward Propagation）是神经网络中的一种计算方法，它将输入数据通过多个神经元进行逐层处理，并得到最终的输出结果。\n",
    "    方向是从输入层开始从左到右一层一层地传播，使每层的输入都是前一层的输出，通常随着越来越靠近输出层，每层的神经元数量会越来越少。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a95cdd7",
   "metadata": {},
   "source": [
    "## 3.1 Inference in Code 在代码中实现\n",
    "    对于下述的一个一层的神经网络例子\n",
    " ![img7](./img/07.png)\n",
    " \n",
    "     Dense函数是Keras中的一种神经网络层类型，也称为全连接层或密集层。它可以在神经网络中添加一个全连接的隐藏层或输出层，实现输入与输出之间的线性变换和非线性激活，从而增强神经网络的表达能力。\n",
    "     \n",
    "     Dense函数的主要参数如下：\n",
    "\n",
    "    units：表示该层中的神经元个数。\n",
    "    activation：表示该层的激活函数类型。\n",
    "    use_bias：表示是否在该层中使用偏置项。\n",
    "    kernel_initializer：表示该层权重的初始化方法。\n",
    "    bias_initializer：表示该层偏置项的初始化方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed83cdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([200.0, 17.0])\n",
    "layer_1 = Dense(units=3, activation='sigmoid')\n",
    "a1 = layer_1(x)\n",
    "\n",
    "layer_2 = Dense(units=1, activation='sigmoid')\n",
    "a2 = layer_2(a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28d4af2",
   "metadata": {},
   "source": [
    "## 3.2 Data in Tensorflow \n",
    "![img8](./img/08.png)\n",
    "\n",
    "    上面两种数据表示的方式是矩阵的方式，是tesorflow更加喜欢的方式\n",
    "    而最后一种是，是一个一维的向量，是我们之前对待数据时使用的方式\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfd0ae7",
   "metadata": {},
   "source": [
    "## 3.3 Building a neural network 搭建一个神经网络\n",
    "    Sequential是Keras中的一种神经网络模型类型，它允许用户将多个神经网络层按顺序叠加在一起，构成一个完整的神经网络模型。Sequential模型类的主要作用是定义神经网络模型的结构，包括输入层、隐藏层和输出层的数量、大小、类型、激活函数等参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2a5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorlfow.keras.models import Sequential\n",
    "import numpy as np\n",
    "\n",
    "layer_1 = Dense(units=3, activation='sigmoid')\n",
    "layer_2 = Dense(units=1, activation='sigmoid')\n",
    "\n",
    "# 根据两层来创建一个模型\n",
    "model = Sequential([layer_1, layer_2])\n",
    "\n",
    "# 训练数据\n",
    "x_train = np.array([200, 17],\n",
    "                   [120, 5],\n",
    "                   [425, 20],\n",
    "                   [212, 18])\n",
    "y_train = np.array([1, 0, 0,1])\n",
    "\n",
    "model.compile(...) #more about this next week\n",
    "model.fit(x_train, y_train) #train it with x and y\n",
    "model.predict(x_new) #to predict x_new\n",
    "\n",
    "# 我们也可以通过以下方式来创建model\n",
    "model_new = Sequential([\n",
    "    Dense(units=25, activation='sigmoid'),\n",
    "    Dense(units=15, activation='sigmoid'),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68b0234",
   "metadata": {},
   "source": [
    "## 4.1 Forward prop in a single layer 单个网络层上的前向传播\n",
    "    解释Lab02 和 Lab03的原理 详见Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3aea72",
   "metadata": {},
   "source": [
    "## 4.2 General implementation of forward propagation 前向传播的一般实现\n",
    "    解释Lab02 和 Lab03的原理 详见Lab\n",
    "    \n",
    "    使用numpy手动实现神经网络的过程\n",
    "![img9](./img/09.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca2129b",
   "metadata": {},
   "source": [
    "## 5.1 Is there a path to AGI? 强人工智能\n",
    "![img10](./img/10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f090f3",
   "metadata": {},
   "source": [
    "## 6.1 How neural networks are implemented efficiently 为什么神经网络如此高效\n",
    "    神经网络使用矩阵计算，GUP非常擅长矩阵计算\n",
    "\n",
    "![img11](./img/11.png)\n",
    "\n",
    "    上述图片，这里使用矩阵代替了繁杂的for循环计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3d8156",
   "metadata": {},
   "source": [
    "## 6.2 Matrix multiplication 矩阵乘法\n",
    "   略 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10174db6",
   "metadata": {},
   "source": [
    "## 6.3 Matrix multiplication rules 矩阵乘法准则\n",
    "略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3368b5e8",
   "metadata": {},
   "source": [
    "## 6.4 Matrix multiplication code 矩阵乘法代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bd9ce9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.   17.   23.    9. ]\n",
      " [-11.  -17.  -23.   -9. ]\n",
      " [  1.1   1.7   2.3   0.9]]\n",
      "[[ 11.   17.   23.    9. ]\n",
      " [-11.  -17.  -23.   -9. ]\n",
      " [  1.1   1.7   2.3   0.9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([[1, -1, 0.1],\n",
    "             [2, -2, 0.2]])\n",
    "AT = A.T\n",
    "\n",
    "W = np.array([[3, 5, 7, 9],\n",
    "             [4, 6, 8 ,0]])\n",
    "\n",
    "z1 = np.matmul(AT, W)\n",
    "print(z1)\n",
    "# another way\n",
    "z2 = AT @ W\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c84f8e",
   "metadata": {},
   "source": [
    "![img12](./img/12.png)\n",
    "\n",
    "    需要注意的是，这里在做矩阵乘法时，需要考虑输出的格式，从而来确定是否对输入使用转置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38f4ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "AT = np.array(p[[200, 17]])\n",
    "W = np.array([[1, -3, 5],\n",
    "               [-2, 4, -6]])\n",
    "b = np.array([[-1, 1, 2]])\n",
    "\n",
    "def dense(AT, W, b, g):\n",
    "    z = AT @ W + b\n",
    "    z_out = g(z)\n",
    "    return a_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
