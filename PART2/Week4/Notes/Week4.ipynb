{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84bbd29f",
   "metadata": {},
   "source": [
    "## 1.1 Decision Tree Model 决策树模型\n",
    "\n",
    "    决策树是一种基于树形结构的分类模型，用于根据事务属性（特征）进行划分和决策。在决策树中，每个节点代表一个特征，每个分支代表特征的一种可能取值，而叶节点则代表对该特征的最终决策。（来自文心一言）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eed9d61",
   "metadata": {},
   "source": [
    "## 1.2 Learning Process 学习过程\n",
    "\n",
    " - Decision 1 : How to choose what feature to split on at each node?\n",
    "     \n",
    "         - Maximize purity(or minimize impurity)\n",
    "     \n",
    " - When do you stop splitting?\n",
    "     \n",
    "         - when a node is 100% one class\n",
    "         - when splitting a node will result in the tree exceeding a maximum depth\n",
    "         - when improvements in purity score are below a threshold(门槛)\n",
    "         - when number of examples in a node is below a threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c01db19",
   "metadata": {},
   "source": [
    "## 2.1 Measuring purity 测量纯度\n",
    "    熵函数\n",
    "    系统越混乱，熵越大\n",
    "    \n",
    "    我们用熵函数的大小来作为测量纯度的一种方法\n",
    "    \n",
    "![img1](./img/01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae3e6d2",
   "metadata": {},
   "source": [
    "## 2.2 Choosing a split: Information Gain 选择拆分信息增益\n",
    "\n",
    "    熵的减少称为信息增益\n",
    "    熵的值越小 纯度越高\n",
    "    \n",
    "![img2](./img/02.png)\n",
    "\n",
    "    最开始的节点处有五猫五狗，所有该处的熵为 H(0.5),后来经过不同的拆分后得到的两个熵函数的值进行加权平均，用开始的值减去该值，得到的就是 熵的减少 ，而熵的减少就是信息的增益，所以选择最后得到的值最大的一种的拆分方式。\n",
    "    \n",
    "\n",
    "得到公式：\n",
    "![img3](./img/03.png)\n",
    "\n",
    "这里的w理解的权重，用于计算加权平均。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b2dbe",
   "metadata": {},
   "source": [
    "## 2.3 Putting it together\n",
    "![img4](./img/04.png)\n",
    "\n",
    "递归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4f6749",
   "metadata": {},
   "source": [
    "## 2.4 Using one-hot encoding of categorical features 独热编码one-hot\n",
    "![img5](./img/05.png) ![img](./img/06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947d6ae4",
   "metadata": {},
   "source": [
    "## 2.5 Continuous valued features 连续值的特征\n",
    "\n",
    "当特征从离散值变得连续，例如体重，是连续的数值\n",
    "\n",
    "我们选择一个数值作为数值的分界线，然而选择的这个数值需要得到最大信息增益\n",
    "![img7](./img/07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501c14f0",
   "metadata": {},
   "source": [
    "## 2.6 Regression Trees(optional) 回归树\n",
    "\n",
    "![img8](./img/08.png)\n",
    "与决策数不同，这里我们需要得到的结果不再是分类，而是预测一个数值\n",
    "\n",
    "如何预测？\n",
    "![img9](./img/09.png)\n",
    "\n",
    "如何拆分？\n",
    "\n",
    "与决策树不同，在回归树中，选择特征进行拆分的目标是找到一个特征和拆分阈值，使得在拆分后的两个子集中，目标变量（连续值）的方差降低最大。换句话说，我们希望找到一个特征和拆分阈值，使得两个子集中的样本具有更好的相似性（即更小的方差）\n",
    "![img10](./img/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603e41b3",
   "metadata": {},
   "source": [
    "## Using multiple decision trees 使用多个决策树\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0382cacf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
